{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must transform data into a state suitable for machine learning.  Whatever transformation we need, the scikit-learn preprocessing module lets us accomplish it in just a couple lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization transforms data to have a mean and unit variance of zero, as expected by many machine learning algorithms.  Therefore, we subtract the mean from each datum and divide all data by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some example training data that we want to scale down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1.0, -1.0, 2.0],\n",
    "                    [2.0, 0.0, 0.0],\n",
    "                    [0.0, 1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a StandardScaler for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81649658, 0.81649658, 1.24721913])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a new, scaled dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this class early in your pipeline to ensure all downstream code is using data of the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some transforms are more than just subtraction and division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping to a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can establish the quantile of each datum by mapping our data onto a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we load and split the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a QuantileTransformer from the preprocessing module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0, n_quantiles = len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create uniformly mapped datasets with our transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results with the percentile module of numpy reveals the results we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3, 5.1, 5.8, 6.5, 7.9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping to a Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms require their data to fit a Gaussian distribution because it minimizes the variance and skewness of the data.  The PowerTransformer class offers the Yeo-Johnson and Box-Cox methods, the latter of which works for negative as well as positive numbers and is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n",
    "X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.28331718, 1.18092228, 0.84160269],\n",
       "       [0.94293279, 1.60960836, 0.3879099 ],\n",
       "       [1.35235668, 0.21715673, 1.09977091]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49024349,  0.17881995, -0.1563781 ],\n",
       "       [-0.05102892,  0.58863195, -0.57612415],\n",
       "       [ 0.69420009, -0.84857822,  0.10051454]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.fit_transform(X_lognormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the QuantileTransformer class to map data onto normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = preprocessing.QuantileTransformer(\n",
    "    output_distribution='normal', random_state=0, n_quantiles = len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = quantile_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.3, 2. , 1. , 0.1],\n",
       "       [4.4, 2.2, 1.1, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.4, 2.2, 1.2, 0.1],\n",
       "       [4.5, 2.3, 1.3, 0.1],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.3, 1.3, 0.2],\n",
       "       [4.6, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.7, 2.4, 1.3, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.8, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.5, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [4.9, 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.6, 1.4, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5. , 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.7, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.2],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.5, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.1, 2.8, 1.6, 0.3],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.6, 0.4],\n",
       "       [5.2, 2.8, 1.7, 0.4],\n",
       "       [5.3, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.8, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.7, 0.4],\n",
       "       [5.4, 2.9, 1.9, 0.5],\n",
       "       [5.4, 2.9, 1.9, 0.6],\n",
       "       [5.4, 2.9, 3. , 1. ],\n",
       "       [5.4, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.3, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.5, 1. ],\n",
       "       [5.5, 2.9, 3.6, 1. ],\n",
       "       [5.5, 2.9, 3.7, 1. ],\n",
       "       [5.5, 3. , 3.8, 1.1],\n",
       "       [5.5, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.1],\n",
       "       [5.6, 3. , 3.9, 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.6, 3. , 4. , 1.2],\n",
       "       [5.7, 3. , 4. , 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.1, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.3, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.3],\n",
       "       [5.8, 3. , 4.4, 1.4],\n",
       "       [5.8, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [5.9, 3. , 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.4],\n",
       "       [6. , 3.1, 4.5, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6. , 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.6, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.1, 4.7, 1.5],\n",
       "       [6.1, 3.2, 4.7, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.5],\n",
       "       [6.2, 3.2, 4.8, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.6],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 4.9, 1.7],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.2, 5. , 1.8],\n",
       "       [6.3, 3.3, 5. , 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.3, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.4, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.1, 1.8],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.2, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.5, 3.4, 5.3, 1.9],\n",
       "       [6.6, 3.4, 5.4, 1.9],\n",
       "       [6.6, 3.4, 5.4, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.4, 5.5, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2. ],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.7, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.5, 5.6, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.8, 3.6, 5.7, 2.1],\n",
       "       [6.9, 3.6, 5.7, 2.2],\n",
       "       [6.9, 3.6, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.2],\n",
       "       [6.9, 3.7, 5.8, 2.3],\n",
       "       [7. , 3.7, 5.9, 2.3],\n",
       "       [7.1, 3.8, 5.9, 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6. , 2.3],\n",
       "       [7.2, 3.8, 6.1, 2.3],\n",
       "       [7.3, 3.8, 6.1, 2.3],\n",
       "       [7.4, 3.8, 6.1, 2.3],\n",
       "       [7.6, 3.9, 6.3, 2.4],\n",
       "       [7.7, 3.9, 6.4, 2.4],\n",
       "       [7.7, 4. , 6.6, 2.4],\n",
       "       [7.7, 4.1, 6.7, 2.5],\n",
       "       [7.7, 4.2, 6.7, 2.5],\n",
       "       [7.9, 4.4, 6.9, 2.5]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_transformer.quantiles_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data to have unit norm enables such quadratic methods as dot-product or a kernel to quantify the separation of data.  The preprocessing module has a method for just this purpose! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = preprocessing.normalize(X, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
